# ğŸ¤– Browser-DPO-Agent

<div align="center">

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1+-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.41.2+-yellow.svg)](https://huggingface.co/transformers/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

ğŸš€ Production-Grade Browser Automation Agent | End-to-End Training Framework Based on Direct Preference Optimization (DPO)

*Enabling AI agents to learn "human preferences" for complex web environment operations*

[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](../README.md) | [ğŸ‡ºğŸ‡¸ English](README_EN.md)

</div>

---

## ğŸ¯ Project Highlights

Browser-DPO-Agent is an **industrial-grade** browser automation agent training framework that leverages cutting-edge **Direct Preference Optimization (DPO)** technology to achieve significant performance improvements on the MiniWoB++ benchmark environment.

### ğŸ”¥ Core Advantages

- ğŸ§  **Intelligent Preference Learning**: Quality-score-based intelligent preference selection algorithm that automatically generates high-quality preference data from environment feedback
- ğŸ—ï¸ **Modular Architecture**: Complete SFTâ†’DPOâ†’Inference training pipeline with flexible expansion and component replacement support
- âš¡ **Production Ready**: Comprehensive error handling, exception classification, MLflow experiment tracking, and model validation mechanisms
- ğŸ›ï¸ **Fine-tuned Optimization**: Hyperparameter configurations based on extensive experiments, effectively preventing overfitting
- ğŸ”§ **Out-of-the-Box**: One-click deployment supporting multiple large language model backends with dynamic element reference management

## ğŸ›ï¸ Technical Architecture

### ğŸ“Š Performance Metrics
- ğŸ¯ **Success Rate Improvement**: 15-25% average success rate improvement on MiniWoB++ tasks after DPO training
- âš¡ **Training Efficiency**: LoRA fine-tuning support, 70% memory reduction, 3x training speed improvement
- ğŸ”„ **Convergence Stability**: Carefully tuned hyperparameters, stable convergence within 10 steps, avoiding overfitting
- ğŸ›¡ï¸ **System Stability**: Comprehensive exception handling and error recovery mechanisms, 99%+ production environment reliability

### ğŸ› ï¸ Technology Stack
```
ğŸ§  AI Framework: PyTorch 2.5.1 + Transformers 4.41.2 + TRL 0.19.1
ğŸ¯ Training Tech: LoRA + DPO + Gradient Accumulation + Early Stopping + Intelligent Preference Selection
ğŸŒ Environment Integration: MiniWoB++ + Selenium 4.21.0 + Gymnasium + Dynamic Element Reference
ğŸ“Š Experiment Management: MLflow + Automated Metrics Tracking + Performance Monitoring
ğŸ”§ Engineering: Poetry + Modular Architecture + Exception Classification + Model Validation
```

### ğŸ—ï¸ Module Architecture
```
src/
â”œâ”€â”€ ğŸ¤– agent/              # Agent Core Module
â”‚   â””â”€â”€ model.py           # Unified Agent Interface + Model Loading Validation
â”œâ”€â”€ ğŸŒ environment/        # Environment Interaction Layer
â”‚   â””â”€â”€ interface.py       # Environment Interface + Element Reference Management + Selector Conversion
â”œâ”€â”€ ğŸ® miniwob/           # MiniWoB++ Environment Integration
â””â”€â”€ ğŸ› ï¸ utils/             # Utilities & Configuration Management
    â”œâ”€â”€ config.py          # Configuration Management + Consistency Validation
    â”œâ”€â”€ mlflow_logger.py   # Experiment Tracking + Fine-grained Exception Handling
    â””â”€â”€ checkpoint_manager.py # Model Checkpoint Management
```

### ğŸ”§ Core Improvement Features

#### âœ… P0-Level Critical Fixes
- **ğŸ¯ Selector Format Unification**: Resolved selector format mismatch between training data and execution environment
- **ğŸ”— Dynamic Element Reference**: Replaced fixed reference values with intelligent element reference management system
- **âš™ï¸ Configuration Consistency**: Ensured complete consistency of DPO configuration between training and inference phases

#### âœ… P1-Level Important Optimizations
- **ğŸ§  Intelligent Preference Selection**: Quality-score-based preference pair selection algorithm improving training data quality
- **ğŸ›¡ï¸ Model Loading Validation**: Comprehensive model and adapter loading validation mechanisms
- **ğŸ” Exception Handling Refinement**: Classified exception handling improving system diagnosis and recovery capabilities

## ğŸš€ Quick Start

### ğŸ“¦ Environment Setup

```bash
# 1. Clone the project
git clone https://github.com/PrescottClub/Browser-DPO-Agent.git
cd Browser-DPO-Agent

# 2. Install dependencies (Poetry recommended)
pip install poetry
poetry install

# 3. Verify environment
python scripts/00_verify_setup.py
```

### âš™ï¸ Configuration

The system uses `config.yaml` for unified configuration management, supporting flexible model and training parameter adjustments:

```yaml
# ğŸ¯ Model Configuration
model:
  base_model_name: "Qwen/Qwen2-7B-Instruct"  # Support any HF model

# ğŸ”§ Training Configuration
training:
  sft:                          # Supervised Fine-tuning Phase
    learning_rate: 2.0e-4
    max_steps: 100
    batch_size: 1
    grad_accumulation_steps: 4
  dpo:                          # DPO Optimization Phase
    learning_rate: 1.0e-6       # ğŸ›ï¸ Fine-tuned learning rate
    max_steps: 10               # âš¡ Fast convergence
    batch_size: 1
    beta: 0.1                   # ğŸ¯ Preference strength control
```

### ğŸ¬ One-Click Training

```bash
# ğŸš€ Complete pipeline (Recommended)
python scripts/run_pipeline.py

# ğŸ”§ Step-by-step execution (Debug mode)
python scripts/01_sft_training.py          # ğŸ“š Supervised Fine-tuning
python scripts/02_collect_preferences.py   # ğŸ¯ Preference Data Collection
python scripts/03_dpo_training.py          # ğŸ§  DPO Preference Optimization
python scripts/04_evaluate_agent.py        # ğŸ“Š Performance Evaluation
```

## ğŸ”¬ Training Process Details

### ğŸ¯ Four-Stage Training Pipeline

```mermaid
graph LR
    A[ğŸ“š SFT Supervised Fine-tuning] --> B[ğŸ¯ Intelligent Preference Collection]
    B --> C[ğŸ§  DPO Preference Optimization]
    C --> D[ğŸ“Š Performance Evaluation]

    A1[Golden Sample Training] --> A
    B1[Quality Scoring Algorithm] --> B
    C1[Preference Contrastive Learning] --> C
    D1[MiniWoB++ Benchmark Testing] --> D
```

1. ğŸ“š **Supervised Fine-tuning (SFT)**: Establish basic capabilities based on expert demonstration data with enhanced model loading validation
2. ğŸ¯ **Intelligent Preference Collection**: Use quality scoring algorithm to select optimal preference pairs from environment feedback
3. ğŸ§  **DPO Optimization**: Optimize decision strategies through high-quality preference contrastive learning
4. ğŸ“Š **Performance Evaluation**: Validate improvement effects on standard test sets with dynamic element reference support

### âš¡ Core Technical Advantages

#### ğŸ§  Intelligent Preference Selection Algorithm
Multi-dimensional quality metric-based preference pair selection mechanism:

- ğŸ¯ **Success Sample Scoring**: Thought process completeness, action format correctness, response length appropriateness
- ğŸ” **Failure Sample Scoring**: Error type identification, failure degree quantification, contrast maximization
- ğŸ“Š **Quality Comparison**: Automatically select highest quality success samples vs lowest quality failure samples
- ğŸ”„ **Fallback Mechanism**: Automatically fallback to random selection when intelligent selection fails

#### ğŸ›ï¸ DPO Hyperparameter Optimization
Optimal configurations validated through extensive experiments, effectively preventing overfitting:

- ğŸ”¥ **Learning Rate**: `1.0e-6` (5x reduction from baseline, ensuring stable convergence)
- âš¡ **Training Steps**: `10 steps` (Fast convergence, avoiding over-optimization)
- ğŸ¯ **Beta Parameter**: `0.1` (Preference strength control, balancing exploration and exploitation)
- ğŸ“Š **Early Stopping**: Best model selection based on validation set

#### ğŸ”— Dynamic Element Reference System
Intelligent element management replacing fixed reference values:

- ğŸ†” **Dynamic Reference Generation**: Generate unique incremental reference IDs for each page element
- ğŸ’¾ **Element Cache Management**: Intelligent caching of element information with reference validation and expiration detection
- ğŸ”„ **Automatic Cleanup Mechanism**: Automatically clean expired references during environment reset, preventing memory leaks
- ğŸ¯ **Selector Conversion**: Support automatic conversion from jQuery-style selectors to standard CSS selectors

#### ğŸ“‹ Preference Data Format
Standard DPO format supporting intelligent data generation:
```json
{
  "prompt": "Click the login button to complete the task",
  "chosen": "thought: I need to locate the login button.\naction: CLICK(selector=\"#login-btn\")",
  "rejected": "error: Unable to find element"
}
```

## ğŸŒ Language Switching

### ğŸ”„ Chinese-English Switching
This project supports bilingual README in Chinese and English, with free switching:

```bash
# ğŸ‡¨ğŸ‡³ Switch to Chinese version
python scripts/switch_language.py --lang zh

# ğŸ‡ºğŸ‡¸ Switch to English version
python scripts/switch_language.py --lang en

# ğŸ“‹ Check current language status
python scripts/switch_language.py --status
```

### ğŸ–±ï¸ Quick Switch
You can also use shortcut scripts:
- Switch to Chinese: Double-click `tools/switch_to_chinese.bat`
- Switch to English: Double-click `tools/switch_to_english.bat`

### ğŸ“ Language File Description
- `README.md` - Currently displayed README file
- `docs/README_ZH.md` - Chinese version backup
- `docs/README_EN.md` - English version backup

## ğŸ”§ System Verification

### ğŸ” Environment Verification
```bash
# ğŸš€ Verify system environment and dependencies
python scripts/00_verify_setup.py

# ğŸ¯ Verify configuration file
python -c "from src.utils.config import load_config; print('âœ… Configuration loading normal')"

# ğŸŒ Verify environment interface
python -c "from src.environment.interface import EnvironmentInterface; print('âœ… Environment interface normal')"
```

## ğŸ“Š Experiment Monitoring

### ğŸ›ï¸ MLflow Experiment Tracking
Integrated enterprise-level experiment management platform with full visualization monitoring:

```bash
# ğŸš€ Start MLflow monitoring dashboard
python start_mlflow_ui.py
# ğŸŒ Access http://localhost:5000
```

### ğŸ“ˆ Core Monitoring Metrics
- **ğŸ“‰ Training Loss Curves**: Real-time monitoring of SFT/DPO training progress
- **âš¡ System Resources**: GPU/CPU/Memory usage tracking
- **ğŸ¯ Performance Metrics**: Success rate, average steps, response time
- **ğŸ”„ Model Versions**: Automatic checkpoint saving and experiment configuration
- **ğŸ“ Git Status**: Code version and reproducibility guarantee

## ğŸ“ Project Structure

```
Browser-DPO-Agent/
â”œâ”€â”€ ğŸ“‹ config.yaml              # ğŸ›ï¸ Unified Configuration Management
â”œâ”€â”€ ğŸ“Š data/                    # ğŸ¯ Training Data & Preference Samples
â”‚   â”œâ”€â”€ sft_golden_samples.jsonl    # Supervised Learning Golden Data
â”‚   â””â”€â”€ preferences/                # DPO Preference Dataset
â”œâ”€â”€ ğŸ¤– models/                  # ğŸ’¾ Model Weight Storage
â”‚   â”œâ”€â”€ sft_v1_adapter/            # SFT LoRA Adapter
â”‚   â””â”€â”€ dpo_v1_adapter/            # DPO LoRA Adapter
â”œâ”€â”€ ğŸš€ scripts/                 # ğŸ”§ Training Pipeline Scripts
â”‚   â”œâ”€â”€ 00_verify_setup.py         # Environment Verification
â”‚   â”œâ”€â”€ 01_sft_training.py         # SFT Training
â”‚   â”œâ”€â”€ 02_collect_preferences.py  # Intelligent Preference Collection
â”‚   â”œâ”€â”€ 03_dpo_training.py         # DPO Training
â”‚   â”œâ”€â”€ 04_evaluate_agent.py       # Performance Evaluation
â”‚   â””â”€â”€ run_pipeline.py           # One-Click Training Pipeline
â”œâ”€â”€ ğŸ§  src/                     # ğŸ’» Core Source Code
â”‚   â”œâ”€â”€ agent/                     # Agent Module
â”‚   â”œâ”€â”€ environment/               # Environment Interaction Layer
â”‚   â”œâ”€â”€ miniwob/                  # MiniWoB++ Integration
â”‚   â””â”€â”€ utils/                    # Utilities
â”œâ”€â”€ ğŸ“Š logs/                    # ğŸ“ Log Files
â””â”€â”€ ğŸ“– README.md               # ğŸ“š Project Documentation
```

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues & Solutions

| Issue Type | Symptoms | Solutions |
|------------|----------|-----------|
| ğŸ”§ Configuration Error | `config.yaml` loading failure | Ensure UTF-8 encoding, check YAML syntax, verify DPO configuration consistency |
| âš¡ Memory Insufficient | CUDA OOM error | Reduce `batch_size` or enable gradient accumulation |
| ğŸ¯ Model Loading Failure | ModelLoadError exception | Check model name format, verify network connection and permissions |
| ğŸ”— Element Reference Error | ElementNotFoundError | Check selector format, confirm page element existence |
| ğŸŒ Environment Dependencies | Module import failure | Run `poetry install` to reinstall |

### ğŸš€ Performance Optimization Recommendations

- ğŸ›ï¸ **Learning Rate Tuning**: Use 1e-6 or smaller learning rates for DPO phase to ensure stable convergence
- âš¡ **Training Steps**: Limit to 10-50 steps for small datasets to prevent overfitting
- ğŸ“Š **Early Stopping**: Enable validation set monitoring for automatic best model selection
- ğŸ” **Real-time Monitoring**: Track key metric changes through MLflow
- ğŸ§  **Preference Quality**: Use intelligent preference selection algorithm to improve training data quality
- ğŸ”— **Element Management**: Enable dynamic element reference management to improve positioning accuracy

## ğŸ¤ Contributing

We welcome community contributions! Please follow these guidelines:

### ğŸ“‹ Development Process
1. ğŸ”€ Fork project â†’ Create feature branch â†’ Submit PR
2. ğŸ§ª Test coverage â†’ Add corresponding test cases for new features
3. ğŸ“š Documentation updates â†’ Synchronously update relevant documentation
4. âœ… Quality checks â†’ Ensure all tests pass and code meets standards

### ğŸ¯ Contribution Areas
- ğŸš€ Performance optimization: Training efficiency, inference speed improvement
- ğŸŒ Environment expansion: Support for more web automation scenarios
- ğŸ§  Algorithm improvement: New preference learning methods
- ğŸ”§ Engineering optimization: Deployment, monitoring, observability

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details

## ğŸŒŸ Acknowledgments

Thanks to the following open source projects for their support:
- [ğŸ¤— Transformers](https://github.com/huggingface/transformers) - Large Language Model Foundation Framework
- [ğŸ¯ TRL](https://github.com/huggingface/trl) - Reinforcement Learning Training Library
- [ğŸ® MiniWoB++](https://github.com/Farama-Foundation/miniwob-plusplus) - Web Automation Benchmark Environment

---

<div align="center">

**ğŸš€ Production-Grade AI Agent Training Framework | Enabling Machines to Learn Human-Preferred Web Operations**

*If this project helps you, please give us a â­Star for support!*

[![GitHub stars](https://img.shields.io/github/stars/PrescottClub/Browser-DPO-Agent?style=social)](https://github.com/PrescottClub/Browser-DPO-Agent/stargazers)

</div>
