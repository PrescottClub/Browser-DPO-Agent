# scripts/03_dpo_training_simple.py
# ç®€åŒ–ç‰ˆDPOè®­ç»ƒè„šæœ¬ï¼Œç”¨äºæµ‹è¯•åŸºæœ¬åŠŸèƒ½

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model

# --- é…ç½® ---
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"
PREFERENCE_DATA_PATH = "data/preferences/dpo_v1_data.jsonl"
DPO_ADAPTER_PATH = "./models/dpo_v1_adapter"

def main():
    print("--- å¯åŠ¨ç®€åŒ–ç‰ˆDPOè®­ç»ƒæµç¨‹ ---")

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæ¯•ã€‚")

    # 2. é…ç½®LoRA
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules="all-linear",
    )
    
    # åº”ç”¨LoRAåˆ°æ¨¡å‹
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 3. åŠ è½½åå¥½æ•°æ®é›†
    print(f"ä» {PREFERENCE_DATA_PATH} åŠ è½½åå¥½æ•°æ®é›†...")
    preference_dataset = load_dataset("json", data_files=PREFERENCE_DATA_PATH, split="train")
    print("æ•°æ®é›†åŠ è½½å®Œæ¯•ã€‚")

    # 4. é…ç½®DPOè®­ç»ƒ
    dpo_config = DPOConfig(
        output_dir=DPO_ADAPTER_PATH,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=2,
        learning_rate=5e-6,
        logging_steps=5,
        max_steps=10,  # å…ˆç”¨10æ­¥æµ‹è¯•
        save_strategy="steps",
        save_steps=5,
        report_to="none",
        beta=0.1,
        max_prompt_length=512,
        max_length=1024,
    )

    # 5. åˆå§‹åŒ–DPOTrainer
    trainer = DPOTrainer(
        model=model,
        ref_model=None,  # TRLä¼šè‡ªåŠ¨åˆ›å»ºå‚è€ƒæ¨¡å‹
        args=dpo_config,
        train_dataset=preference_dataset,
        processing_class=tokenizer,
    )

    # 6. å¼€å§‹è®­ç»ƒ
    print("--- å¼€å§‹DPOè®­ç»ƒ ---")
    try:
        trainer.train()
        print(f"--- DPOè®­ç»ƒå®Œæˆï¼ŒAdapterå·²ä¿å­˜è‡³ {DPO_ADAPTER_PATH} ---")
        print("ğŸ‰ DPOè®­ç»ƒæˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ DPOè®­ç»ƒå¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ˜¾å­˜ä¸è¶³æˆ–å…¶ä»–é…ç½®é—®é¢˜ã€‚")

    print("--- DPOè®­ç»ƒæµç¨‹ç»“æŸ ---")

if __name__ == "__main__":
    main()
