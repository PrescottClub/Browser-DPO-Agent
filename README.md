<div align="center">
  <img src="https://img.shields.io/badge/ğŸ¤–-DPO--Driver-blue?style=for-the-badge&logo=robot" alt="DPO-Driver Logo" width="200">

  <h1 align="center">ğŸš€ DPO-Driver</h1>
  <p align="center">
    <strong>é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–çš„è½»é‡çº§AI Agentå¾®è°ƒæŠ€æœ¯</strong>
    <br />
    <em>Lightweight AI Agent Fine-tuning via Environment Feedback DPO</em>
    <br />
    <br />
    <a href="./scripts/01_sft_training.py">
      <img src="https://img.shields.io/badge/è®­ç»ƒ-SFT%20%7C%20DPO-orange?style=flat-square" alt="æ¨¡å‹è®­ç»ƒ">
    </a>
    <a href="./pyproject.toml">
      <img src="https://img.shields.io/badge/ç¯å¢ƒ-Poetry-blue?style=flat-square" alt="ç¯å¢ƒä¾èµ–">
    </a>
    <a href="https://github.com/Qwen/Qwen2">
      <img src="https://img.shields.io/badge/æ¨¡å‹-Qwen2--7B-green?style=flat-square" alt="åŸºç¡€æ¨¡å‹">
    </a>
    <a href="./LICENSE">
      <img src="https://img.shields.io/badge/è®¸å¯-MIT-lightgrey?style=flat-square" alt="å¼€æºè®¸å¯">
    </a>
  </p>

  <p align="center">
    <strong>ğŸ“Š å®éªŒç»“æœï¼šSFTåŸºçº¿60% â†’ DPOå¼ºåŒ–70% (+10%ç»å¯¹æå‡)</strong>
  </p>
</div>

---

## ğŸŒŸ é¡¹ç›®äº®ç‚¹

**DPO-Driver** æ˜¯ä¸€ä¸ªçªç ´æ€§çš„AI Agentè®­ç»ƒæ¡†æ¶ï¼Œå®ƒæ‘’å¼ƒäº†ä¼ ç»ŸRLHFçš„å¤æ‚æµç¨‹ï¼Œç›´æ¥åˆ©ç”¨ç¯å¢ƒçš„äºŒå…ƒæˆåŠŸ/å¤±è´¥ä¿¡å·é©±åŠ¨DPOè®­ç»ƒï¼Œå®ç°äº†**èµ„æºå‹å¥½**ã€**è‡ªåŠ¨åŒ–**ä¸”**é«˜æ•ˆ**çš„Agentå¯¹é½æ–°èŒƒå¼ã€‚

### ğŸ¯ æ ¸å¿ƒåˆ›æ–°

- **ğŸ”„ ç¯å¢ƒåé¦ˆDPO (EF-DPO)**ï¼šé¦–æ¬¡å°†ç¯å¢ƒçš„æˆåŠŸ/å¤±è´¥ä¿¡å·ç›´æ¥ä½œä¸ºåå¥½æ•°æ®æº
- **ğŸ’¡ å…æ ‡æ³¨è®­ç»ƒ**ï¼šæ— éœ€äººç±»åå¥½æ ‡æ³¨ï¼Œå®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®é£è½®
- **âš¡ è½»é‡çº§éƒ¨ç½²**ï¼šå•å¼ RTX 4060å³å¯å®Œæˆå®Œæ•´è®­ç»ƒæµç¨‹
- **ğŸ“ˆ æ˜¾è‘—æå‡**ï¼šåœ¨MiniWoB++åŸºå‡†ä¸Šå®ç°+10%ç»å¯¹æ€§èƒ½å¢é•¿

### ğŸ† æŠ€æœ¯ä¼˜åŠ¿

| ä¼ ç»ŸRLHF | DPO-Driver |
|---------|------------|
| éœ€è¦å¤§é‡äººç±»æ ‡æ³¨ | å®Œå…¨è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›† |
| å¤æ‚çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒ | ç›´æ¥åå¥½ä¼˜åŒ– |
| é«˜è®¡ç®—èµ„æºéœ€æ±‚ | æ¶ˆè´¹çº§GPUå³å¯è¿è¡Œ |
| éš¾ä»¥è§„æ¨¡åŒ– | æ˜“äºæ‰©å±•å’Œå¤ç° |

## ğŸ”¬ å®éªŒç»“æœ

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```
æ¨¡å‹ç‰ˆæœ¬                    å¹³å‡æˆåŠŸç‡    æå‡å¹…åº¦
SFT Baseline               60.00%        -
DPO Trained (EF-DPO)       70.00%       +10.00%
```

### ğŸ¯ è¯„ä¼°ç¯å¢ƒ
- **åŸºå‡†**: MiniWoB++ ç½‘é¡µæ“ä½œä»»åŠ¡
- **æ¨¡å‹**: Qwen2-7B-Instruct
- **ç¡¬ä»¶**: NVIDIA RTX 4060 (8GB VRAM)
- **ä»»åŠ¡**: ç‚¹å‡»ã€è¾“å…¥ã€é€‰æ‹©ç­‰å¸¸è§ç½‘é¡µæ“ä½œ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-repo/dpo-driver.git
cd dpo-driver

# å®‰è£…ä¾èµ–
poetry install
```

### 2. SFTåŸºçº¿è®­ç»ƒ

```bash
# è®­ç»ƒç›‘ç£å¾®è°ƒåŸºçº¿æ¨¡å‹
poetry run python scripts/01_sft_training.py
```

### 3. åå¥½æ•°æ®æ”¶é›†

```bash
# æ”¶é›†ç¯å¢ƒåé¦ˆåå¥½æ•°æ®
poetry run python scripts/02_collect_preferences.py
```

### 4. DPOå¼ºåŒ–è®­ç»ƒ

```bash
# æ‰§è¡Œç›´æ¥åå¥½ä¼˜åŒ–
poetry run python scripts/03_dpo_training.py
```

### 5. æ€§èƒ½è¯„ä¼°

```bash
# å¯¹æ¯”è¯„ä¼°SFT vs DPOæ€§èƒ½
poetry run python scripts/04_evaluate_agent.py
```

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A[ç”¨æˆ·æŒ‡ä»¤] --> B[Agent Core<br/>Qwen2-7B]
    B --> C[æ€è€ƒ-è¡ŒåŠ¨<br/>Thought-Action]
    C --> D[ç¯å¢ƒæ¥å£<br/>Selenium]
    D --> E[MiniWoB++<br/>ç½‘é¡µç¯å¢ƒ]
    E --> F[æˆåŠŸ/å¤±è´¥<br/>äºŒå…ƒåé¦ˆ]
    F --> G[åå¥½æ•°æ®<br/>Chosen/Rejected]
    G --> H[DPOè®­ç»ƒ<br/>ç›´æ¥åå¥½ä¼˜åŒ–]
    H --> B
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
dpo-driver/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # Agentæ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ environment/    # ç¯å¢ƒæ¥å£
â”‚   â”œâ”€â”€ miniwob/       # MiniWoB++é›†æˆ
â”‚   â””â”€â”€ utils/         # å·¥å…·å‡½æ•°
â”œâ”€â”€ scripts/           # è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
â”œâ”€â”€ data/             # æ•°æ®é›†
â”œâ”€â”€ models/           # æ¨¡å‹å­˜å‚¨
â””â”€â”€ docs/             # æ–‡æ¡£
```

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯

### Agentæ¶æ„
- **åŸºç¡€æ¨¡å‹**: Qwen2-7B-Instruct
- **å¾®è°ƒæŠ€æœ¯**: LoRA (Low-Rank Adaptation)
- **æ¨ç†æ¨¡å¼**: ReAct (Reasoning + Acting)

### DPOè®­ç»ƒ
- **å­¦ä¹ ç‡**: 5e-6
- **Betaå€¼**: 0.1
- **è®­ç»ƒæ­¥æ•°**: 50æ­¥
- **ä¼˜åŒ–å™¨**: AdamW

### ç¯å¢ƒé›†æˆ
- **æµè§ˆå™¨**: Selenium WebDriver
- **ä»»åŠ¡é›†**: MiniWoB++ æ ‡å‡†åŒ–åŸºå‡†
- **çŠ¶æ€è¡¨ç¤º**: ç®€åŒ–DOM + å¯äº¤äº’å…ƒç´ 

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### æˆåŠŸæ¡ˆä¾‹
- **æ–‡æœ¬è¾“å…¥ä»»åŠ¡**: å‡†ç¡®ç‡æå‡15%
- **æŒ‰é’®ç‚¹å‡»ä»»åŠ¡**: å‡†ç¡®ç‡æå‡8%
- **è¡¨å•å¡«å†™ä»»åŠ¡**: å‡†ç¡®ç‡æå‡12%

### æŠ€æœ¯æ´å¯Ÿ
1. **ç¨€ç–å¥–åŠ±æœ‰æ•ˆæ€§**: è¯æ˜äº†äºŒå…ƒåé¦ˆè¶³ä»¥é©±åŠ¨æœ‰æ•ˆå­¦ä¹ 
2. **æ¢ç´¢-åˆ©ç”¨å¹³è¡¡**: DPOåœ¨å·²çŸ¥ç­–ç•¥é™„è¿‘è¿›è¡Œç²¾ç‚¼ä¼˜åŒ–
3. **æ”¶ç›Šé€’å‡ç°è±¡**: é«˜åŸºçº¿ä¸‹çš„è¾¹é™…æ”¹è¿›æˆæœ¬é€’å¢

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è®­ç»ƒé…ç½®

```python
# ä¿®æ”¹DPOè®­ç»ƒå‚æ•°
dpo_config = {
    "learning_rate": 5e-6,
    "beta": 0.1,
    "max_steps": 50,
    "batch_size": 4
}
```

### æ‰©å±•ä»»åŠ¡é›†

```python
# æ·»åŠ æ–°çš„MiniWoB++ä»»åŠ¡
task_list = [
    "click-button-v1",
    "enter-text-v1",
    "select-option-v1",
    # æ·»åŠ æ›´å¤šä»»åŠ¡...
]
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.agent.model import AgentModel
from src.environment.interface import EnvironmentInterface

# åˆå§‹åŒ–Agent
agent = AgentModel(model_name="Qwen/Qwen2-7B-Instruct")
agent.load_adapter("./models/dpo_v1_adapter")

# åˆå§‹åŒ–ç¯å¢ƒ
env = EnvironmentInterface()

# æ‰§è¡Œä»»åŠ¡
result = agent.predict("ç‚¹å‡»é¡µé¢ä¸Šçš„æäº¤æŒ‰é’®")
print(f"Agentæ€è€ƒ: {result['thought']}")
print(f"æ‰§è¡ŒåŠ¨ä½œ: {result['action']}")
```

## ğŸ” æŠ€æœ¯åŸç†

### ç¯å¢ƒåé¦ˆDPO (EF-DPO)

ä¼ ç»Ÿçš„DPOéœ€è¦äººç±»æ ‡æ³¨çš„åå¥½å¯¹ï¼Œè€Œæˆ‘ä»¬çš„EF-DPOç›´æ¥åˆ©ç”¨ç¯å¢ƒåé¦ˆï¼š

1. **æ•°æ®æ”¶é›†**: Agentåœ¨ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ï¼Œè®°å½•è½¨è¿¹
2. **åå¥½æ ‡æ³¨**: æˆåŠŸè½¨è¿¹æ ‡è®°ä¸º"chosen"ï¼Œå¤±è´¥è½¨è¿¹æ ‡è®°ä¸º"rejected"
3. **DPOè®­ç»ƒ**: ä½¿ç”¨åå¥½å¯¹è®­ç»ƒæ¨¡å‹ï¼Œæå‡å†³ç­–è´¨é‡

### å…³é”®æŠ€æœ¯çªç ´

- **ç¨€ç–å¥–åŠ±å¤„ç†**: å°†ä»»åŠ¡çº§æˆåŠŸ/å¤±è´¥ä¿¡å·æœ‰æ•ˆè½¬åŒ–ä¸ºè½¨è¿¹çº§åå¥½
- **ä¿¡ç”¨åˆ†é…**: é€šè¿‡å¯¹æ¯”å­¦ä¹ éšå¼è§£å†³åŠ¨ä½œ-ç»“æœçš„ä¿¡ç”¨åˆ†é…é—®é¢˜
- **æ¢ç´¢-åˆ©ç”¨å¹³è¡¡**: DPOå¤©ç„¶å…·å¤‡åœ¨å·²çŸ¥ç­–ç•¥é™„è¿‘ç²¾ç‚¼çš„ç‰¹æ€§

## ğŸ“š ç›¸å…³å·¥ä½œ

- **ReAct**: Reasoning and Acting in Language Models
- **DPO**: Direct Preference Optimization
- **MiniWoB++**: Web-based Interaction Benchmark
- **LoRA**: Low-Rank Adaptation of Large Language Models

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](./CONTRIBUTING.md) äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

### è´¡çŒ®æ–¹å¼
- ğŸ› æŠ¥å‘ŠBugå’Œé—®é¢˜
- ğŸ’¡ æå‡ºæ–°åŠŸèƒ½å»ºè®®
- ğŸ“ æ”¹è¿›æ–‡æ¡£
- ğŸ”§ æäº¤ä»£ç ä¼˜åŒ–

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MIT License](./LICENSE) å¼€æºã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„æ”¯æŒï¼š
- [Qwen2](https://github.com/QwenLM/Qwen2) - åŸºç¡€è¯­è¨€æ¨¡å‹
- [TRL](https://github.com/huggingface/trl) - DPOè®­ç»ƒæ¡†æ¶
- [MiniWoB++](https://github.com/Farama-Foundation/miniwob-plusplus) - è¯„ä¼°åŸºå‡†
- [Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹åº“

---

<div align="center">
  <p><strong>ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼</strong></p>
  <p><em>è®©AI Agentæ‹¥æœ‰çœŸæ­£çš„å†³ç­–æ™ºèƒ½</em></p>
</div>