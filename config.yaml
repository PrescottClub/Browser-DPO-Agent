# DPO-Driver V1.0 - Central Configuration

# Project settings
project:
  seed: 42

model:
  base_model_name: "Qwen/Qwen2-7B-Instruct"
  # quantization_config (future use)
  
paths:
  sft_data: "data/sft_golden_samples.jsonl"
  preference_data: "data/preferences/dpo_v1_data.jsonl"
  sft_adapter_path: "./models/sft_v1_adapter"
  dpo_adapter_path: "./models/dpo_v1_adapter"

training:
  sft:
    learning_rate: 2.0e-4
    max_steps: 100
    batch_size: 1
    grad_accumulation_steps: 4
  dpo:
    learning_rate: 1.0e-6
    max_steps: 10
    batch_size: 1
    grad_accumulation_steps: 2
    beta: 0.1

evaluation:
  num_episodes_per_task: 20
  tasks:
    - "click-button"
    - "click-test"
